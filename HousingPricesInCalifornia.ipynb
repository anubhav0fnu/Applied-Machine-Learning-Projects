{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#   **Problem Statement** : \n",
    "------\n",
    "<blockquote>\n",
    "        <p>To build a model that learn from **Califoria Census Data** and be able to predict the median housing prices in any district in California given all other metrics. \n",
    "        </p>\n",
    "</blockquote>\n",
    " [Housing Dataset](http://lib.stat.cmu.edu/datasets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Project check-List-(**Start from here ALWAYS**)\n",
    "------\n",
    "<ul>\n",
    "    <li>look at the big picture.</li>\n",
    "    <li>Get the data.</li>\n",
    "    <li>Discover and visualize the data to gain insights.</li>\n",
    "    <li>Prepare the data for Machine Learning algorithms.</li>\n",
    "    <li>Select a model and train it.</li>\n",
    "    <li>Fine-tune your model.</li>\n",
    "    <li>Present your solution.</li>\n",
    "    <li>Launch, monitor, and maintain your system.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the big pucture.\n",
    "<blockquote>\n",
    "        <ul>\n",
    "            <li><h3>While Framing a problem. Ask following ?</h3>\n",
    "               <ul><li>\n",
    "               What is the business objective.</li>\n",
    "               <li>\n",
    "                What current solutions look like(if any)\n",
    "               </li></ul>\n",
    "               **Answer:**\n",
    "               <p>Okay, with all this information you are now ready to start designing your system. it is clearly a typical __***supervised learning***__ task since you are given labeled training examples (each instance comes with the expected output, i.e., the district’s median housing price).</p>\n",
    "               <p>Moreover, it is also a typical regression task, since you are asked to predict a value. More specifically, this is a __***multivariate regression***__ problem since the system will use multiple features to make a prediction (it will use the district’s population, the median income, etc.).</p>\n",
    "               <p>there is no continuous flow of data coming in the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain __***batch learning***__ should do just fine</p>\n",
    "            </li>\n",
    "            <li><h3>Select the performance Measure.?</h3>\n",
    "                <p>A typical performance measure for regression problems is the __***Root Mean Square Error (RMSE)***__. It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors.RMSE(X,h) is the cost function measured on the set of examples using your hypothesis h.\n",
    "                </p>\n",
    "                <p>Suppose that there are many __***outlier districts***__. In that case, you may consider using the __***Mean Absolute Error***__ (also called the Average Absolute Deviation</p>\n",
    "               <blockquote>__***The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE(L2) is more sensitive to outliers than the MAE(L1). But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.***__</blockquote>\n",
    "            </li>\n",
    "            <li><h3>Check the Assumptions?</h3>\n",
    "            <p>Check whether the predition output from your system needed to be in numeric format or categorical format(\"cheap\",\"medium\",\"expensive\") which will be used by another downstream system. Based on this assumption, you need to change the model from regression to classification.\n",
    "            </p>\n",
    "            </li>\n",
    "        </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data & Familize yourself with the data schema\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Automate the data fetching process.\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_PATH =\"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH  =\"datasets/housing\"\n",
    "HOUSING_URL   = DOWNLOAD_PATH + HOUSING_PATH + \"/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path= os.path.join(housing_path,\"/housing.tgz\")\n",
    "    \n",
    "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
    "    \n",
    "    housing_tgz=tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data to pandas & quick look at the Data Structure( Df ).\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path,'housing.csv')\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0    -122.23     37.88                41.0        880.0           129.0   \n",
      "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
      "2    -122.24     37.85                52.0       1467.0           190.0   \n",
      "3    -122.25     37.85                52.0       1274.0           235.0   \n",
      "4    -122.25     37.85                52.0       1627.0           280.0   \n",
      "\n",
      "   population  households  median_income  median_house_value ocean_proximity  \n",
      "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
      "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
      "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
      "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
      "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
      "CPU times: user 37 ms, sys: 6.27 ms, total: 43.3 ms\n",
      "Wall time: 42.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "housing = load_housing_data()\n",
    "print(housing.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (20640, 10) \n",
      " ['longitude' 'latitude' 'housing_median_age' 'total_rooms' 'total_bedrooms'\n",
      " 'population' 'households' 'median_income' 'median_house_value'\n",
      " 'ocean_proximity'] \n",
      "\n",
      "****************************************************************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      "longitude             20640 non-null float64\n",
      "latitude              20640 non-null float64\n",
      "housing_median_age    20640 non-null float64\n",
      "total_rooms           20640 non-null float64\n",
      "total_bedrooms        20433 non-null float64\n",
      "population            20640 non-null float64\n",
      "households            20640 non-null float64\n",
      "median_income         20640 non-null float64\n",
      "median_house_value    20640 non-null float64\n",
      "ocean_proximity       20640 non-null object\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(type(housing),housing.shape,\"\\n\", housing.columns.values,\"\\n\") #print(data.head(n=1))\n",
    "print('*' *100)\n",
    "housing.info()\n",
    "print('*' *100)\n",
    "# the \"total_bedrooms\" attribute has 20433  non-null value, so we need to take care of \"missing values.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-30d0e1cba5b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhousing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"h\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Set2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mboxplot\u001b[0;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth, whis, notch, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2198\u001b[0;31m     \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, boxplot_kws)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxplot_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;34m\"\"\"Make the plot.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_boxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxplot_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"h\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mdraw_boxplot\u001b[0;34m(self, ax, kws)\u001b[0m\n\u001b[1;32m    471\u001b[0m                                          \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                                          \u001b[0mwidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                                          **kws)\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestyle_boxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mboxplot\u001b[0;34m(self, x, notch, sym, vert, whis, positions, widths, patch_artist, bootstrap, usermedians, conf_intervals, meanline, showmeans, showcaps, showbox, showfliers, boxprops, labels, flierprops, medianprops, meanprops, capprops, whiskerprops, manage_xticks, autorange, zorder)\u001b[0m\n\u001b[1;32m   3272\u001b[0m             \u001b[0mbootstrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxplot.bootstrap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m         bxpstats = cbook.boxplot_stats(x, whis=whis, bootstrap=bootstrap,\n\u001b[0;32m-> 3274\u001b[0;31m                                        labels=labels, autorange=autorange)\n\u001b[0m\u001b[1;32m   3275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnotch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m             \u001b[0mnotch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxplot.notch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36mboxplot_stats\u001b[0;34m(X, whis, bootstrap, labels, autorange)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m         \u001b[0;31m# arithmetic mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m         \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;31m# medians and quartiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2909\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.boxplot(x=housing,orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MIN. , MAX. and number of unique values in each column.\n",
    "dum1,dum2,dum3,dum4,dum5 = housing.longitude.unique() , housing.latitude.unique() , housing.housing_median_age.unique() ,housing.ocean_proximity.unique(), housing.population.unique()\n",
    "print(housing.longitude.min(),\"-->\",housing.longitude.max(),len(dum1),\"\\n\")\n",
    "print(housing.latitude.min(),\"-->\",housing.latitude.max(),len(dum2),\"\\n\")\n",
    "print(housing.housing_median_age.min(),\"-->\", housing.housing_median_age.max(),len(dum3),\"\\n\")\n",
    "print(housing.ocean_proximity.min(),\"-->\", housing.ocean_proximity.max(),len(dum4),\"\\n\",np.sort(dum4),\"\\n\")\n",
    "print(housing.population.min(),\"-->\", housing.population.max(),len(dum5),\"\\n\",np.sort(dum5),\"\\n\")\n",
    "print('*' *100)\n",
    "# to find how many categories exist for attribute \"ocean_proximity\" and associated rows.\n",
    "# we infer: How many districts for each category.\n",
    "print(housing[\"ocean_proximity\"].value_counts())\n",
    "\n",
    "#or \n",
    "print(housing.groupby('ocean_proximity').size())\n",
    "print('*' *100)\n",
    "print(housing.describe())\n",
    "\n",
    "#understanding:\n",
    "# 1.std :: measures how dispersed the values.\n",
    "# 2.percentiles ::\n",
    "# 25% of the districts(#of rows) have feature \" housing_median_age\" lower than 18. --> 1st quartile\n",
    "# 50% of the districts(#of rows) have feature \" housing_median_age\" lower than 29  \n",
    "# 75% of the districts(#of rows) have feature \" housing_median_age\" lower than 37  -->3rd quartile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to visualize the data.\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-Feature Relationships\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#df = pd.DataFrame(housing, columns=['logitude', 'latitude', 'median_income', 'population'])\n",
    "\n",
    "sm = scatter_matrix(housing, alpha=0.2, figsize=(15, 15), diagonal='kde')\n",
    "\n",
    "#Change label rotation\n",
    "[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "\n",
    "#May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-0.3,0.5) for s in sm.reshape(-1)]\n",
    "\n",
    "#Hide all ticks\n",
    "[s.set_xticks(()) for s in sm.reshape(-1)]\n",
    "[s.set_yticks(()) for s in sm.reshape(-1)]\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Test set\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    \n",
    "    test_set_size = int(len(data)*test_ratio)\n",
    "    test_indices  = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    \n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set),\"train_set ,\",len(test_set),\"test_set\")\n",
    "#just to check whether the test_set and train_set is changing after every execution.--it is..!!\n",
    "print(train_set.head(n=2))\n",
    "print(test_set.head(n=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35;46m NOTE: if you execute the above program again then it will change the test_set.','\n",
      "',' As you train and test you ML algorithm repeatedly. The algorithm will get to see almost the whole dataset. This does not generalise(not really getting low generalization error).','\n",
      "','To avoid it either use no.random.seed(42) before calling np.random.permutation() so that it always generates the same shuffled indices. or save the test_set on the first run and then load it in subsequent runs\n"
     ]
    }
   ],
   "source": [
    "print (\"\\033[1;35;46m NOTE: if you execute the above program again then it will change the test_set.','\\n',' As you train and test you ML algorithm repeatedly. The algorithm will get to see almost the whole dataset. This does not generalise(not really getting low generalization error).','\\n','To avoid it either use no.random.seed(42) before calling np.random.permutation() so that it always generates the same shuffled indices. or save the test_set on the first run and then load it in subsequent runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_set.head(n=2))\n",
    "print(test_set.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[flexible-apply](http://pandas.pydata.org/pandas-docs/version/0.17.1/groupby.html)\n",
    "\n",
    "[pandas.DataFrame.loc](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "print (\"\\033[1;35;46m the above solutions do not work when we fetch a updated dataset.\\n SOLUTION: to use each instance's identifier to decide whether or not it should go to test_set.{Assuming that the instances have a unique and immutable identifier. \\n we could compute a hash of each instance's identifier, keep only the last two bytes of the hash, and put the instance in the test set if value is <= 256*test_ratio i.e 51} \\n  This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test_set will contain 20% of the new instances,but it will not contain any instance that was previosly in the train_set \")\n",
    "def test_set_check(identifier, test_ratio, hash):\n",
    "    return hash(np.int64(identifier)).digest()[-1] < test_ratio*256\n",
    "\n",
    "def split_train_test(data, test_ratio,id_column , hash=hashlib.md5):\n",
    "    #print(type(data))\n",
    "    ids=data[id_column]\n",
    "    #print(type(ids)) #print(ids.head())\n",
    "    in_test_set=ids.apply(lambda id_ : test_set_check(id_,test_ratio,hash))\n",
    "    #print(type(in_test_set)) #print(in_test_set.head()) #print(~in_test_set.head()) #print(data.loc[~in_test_set].head()) #print(data.loc[in_test_set].head())\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set] # all False value are in train and Tre\n",
    "\n",
    "housing_with_ID= housing.reset_index() # add an 'index' column\n",
    "#print(housing_with_ID.head())\n",
    "train_set, test_set = split_train_test(housing_with_ID, 0.2, \"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"\\033[1;35;46m NOTE: if you use row index as the UNIQUE IDENTIFIER, then make sure that \\n 1. new data must get appended to the end of the dataset. \\n 2. no row ever get deleted.\\n NOTE: if you we not able to make the above approach possoible, select a UNIQUE IDENTIFIER from the most stable features. eg. district longitude and latitude are 100% stable,so combime them to useas an ID \\n Instead we can use most stable features to build unique identifiers\\n here we can combine logitude and latitude that are garrunted to be stable for a few million years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_with_ID[\"id\"]= housing[\"longitude\"]*1000 + housing[\"latitude\"]\n",
    "\n",
    "train_set, test_set = split_train_test(housing_with_ID, 0.2, \"id\")\n",
    "train_set.shape,    test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### using sckit-learn for above task -----RANDOM SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print (\"\\033[1;32;43m QUESTION: you can pass this function multiple datasets with an identical number of rows, and it will split them on the same indices--THIS APPROACH IS USEFULL IF YOU HAVE A SEPERATE DATAFRAME FOR LABELS. \\n\")\n",
    "train_set,test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "train_set.shape,    test_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"\\033[1;35;46m 1. It is important to use a training set that is representative of the cases you want to generalise to. \\n if the sample is too small you will have a SAMPLING NOISE {I.E THE NONREPRESENTATIVE DATA AS A RESULT OF CHANCE.}. with non-representative training set, we will train a model that is unlikely to make accurate predictions.\\n\")\n",
    "print (\"\\033[1;30;46m But, even very large samples can be non-representative if the sampling method is flawed :: SAMPLING BIAS   \\n\")\n",
    "print (\"\\033[1;35;46m So far we have considered purely RANDOM SAMPLING METHODS. This works if you have large dataset(especially related to # of attributes but, if it is not than you run into introducing SIGNIFICANT SAMPLING BIAS.)  \\n\")\n",
    "print (\"\\033[1;30;46m Eg, when a survey company decides to call 1000 people to ask them few questions, they do not just pick just 1000 random people.\\n they try to ensure that these 1000 people are representative of the whole population.\\n   \\n\")\n",
    "print (\"\\033[1;35;46m suppose US population has 51.3% FEMALE and 48.7% MALE, so well conducted survey in US would try to maintain this ratio in the SAMPLE : 513 FEMALES and 487 MALES. \\n this is called STRATIFIED SAMPLING.\\n THE POPULATION IS DEVIDED INTO HOMOGENEOUS SUBGROUPS CALLED STRATA and, THE RIGHT NUMBER OF INSTANCES IS SAMPLED FROM EACH STRATUM TO GAURANTEE THAT THE testset IS REPRESENTATIVE OF WHOLE POPULATION. \\n\")\n",
    "\n",
    "print (\"\\033[1;30;46m if PURE RANDOM SAMPLING IS USED THEN THERE WOULD BE 12% CHANCE OF SAMPLING A SKEWED testset WITH EITHER 49% FEMALE or 54% FEMALE.\\n either way the survey results would be significantly biased.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppose, experts said that \"median_income\" is a very important attribute to predict median housing pricces.\n",
    "# you many want to ensure that the testset is representative of various categories of incomes in the whole dataset.\n",
    "dum10 = housing.median_income.unique()\n",
    "print(housing.median_income.min(),\"-->\", housing.median_income.max(),len(dum10),housing['median_income'].count(),\"\\n\",np.sort(dum10),\"\\n\")\n",
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "housing['median_income'].plot.hist(bins=50, figsize=(6,6))\n",
    "plt.show()\n",
    "print (\"\\033[1;32;43m QUESTION: How to see  histogram more closely \\n most 'median housing' values are clustered between 2-5 but, some 'median income' go far beyond 6\\n\")\n",
    "print (\"\\033[1;35;46m Therefore, your dataset must contain sufficient number of instances from each stratum, otherwise the estimate(coeff.) of the stratum's importance may be biased. This means that you should not have too many stratas, and each stratum should be large enough\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"median_income min max\",housing.median_income.min(),\"-->\", housing.median_income.max(),\"\\n\")\n",
    "print (\"\\033[1;35;46m 1. divide by 1.5 & taken CEIL() to limit the number of 'income_categories' \\n 2. then merging all categories > 5\\n\")\n",
    "\n",
    "#print(housing['median_income'].tail(),\"\\n\")\n",
    "housing['income_cat']= np.ceil(housing['median_income'] / 1.5)\n",
    "#print(\"income_cat min max\",housing['income_cat'].min(),\"-->\", housing['income_cat'].max(),\"\",housing['income_cat'].unique() )\n",
    "#print(housing['income_cat'].tail())\n",
    "#whereverm the condition is FALSE replace the entry by \"5\" in inplace manner. {df.where(cond, change..?)}\n",
    "#it is quivalent to np.where(condition, src, change..?)\n",
    "housing['income_cat'].where(housing['income_cat'] < 5, 5, inplace='True') \n",
    "\n",
    "#print(\"income_cat min max\",housing['income_cat'].min(),\"-->\", housing['income_cat'].max(),\"\",housing['income_cat'].unique() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now you can do STRATIFIED SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(housing,housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set  = housing.loc[test_index]\n",
    "    \n",
    "strat_train_set.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#housing.info() # now has 11 columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_train_set, new_test_set = train_test_split(housing,test_size=0.2, random_state=42)\n",
    "\n",
    "RANDOM     =new_test_set['income_cat'].value_counts() / len(new_test_set)    # TEST SET WITH RANDOM SAMPLING\n",
    "OVERALL    =housing['income_cat'].value_counts() / len(housing)              #---overall DATA SET\n",
    "STRATIFIED =strat_test_set['income_cat'].value_counts() / len(strat_test_set)# TEST SET WITH STRATIFIED SAMPLING \n",
    "\n",
    "err1 = (OVERALL-RANDOM)*100\n",
    "err2 = (OVERALL-STRATIFIED)*100\n",
    "\n",
    "d = {'OVERALL':OVERALL,'RANDOM':RANDOM,'STRATIFIED':STRATIFIED,'Rand.%error':err1,'Strat.%error':err2}\n",
    "compare=pd.DataFrame(data=d,columns=['OVERALL','RANDOM','STRATIFIED','Rand.%error','Strat.%error'] )\n",
    "table  =compare.sort_index()\n",
    "print (\"\\033[1;35;46m Observation: test_set generated by STRATFIED SAMPLING has income category proportions almost identical to those in the full dataset\\n whereas, the test_set  generated by pure RANDOM SAMPLING is quite SKEWED \\n\")\n",
    "\n",
    "from IPython.display import display\n",
    "display(table)\n",
    "print (\"\\033[1;32;43m QUESTION: Think of how to display Skewness \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now remove the \"income_cat\" from the housing dataset to make it to normal.\n",
    "\n",
    "strat_train_set.info()\n",
    "for set in (strat_train_set,strat_test_set):\n",
    "    set.drop(['income_cat'],axis=1,inplace='True')\n",
    "strat_train_set.info()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCOVER & VISUALIZE THE DATA TO GAIN INSIGHT\n",
    "------\n",
    "Make sure that you have kept the data aside and you are only exploring the training set.\n",
    "if the training set is very large, You may want to sample and EXPLORATION SET, to male manipulation easy and fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a copy of training set.\n",
    "\n",
    "housing=strat_train_set.copy()\n",
    "#housing.info()       # its training set only, not full set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### visualizing the geographical Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude')\n",
    "plt.show()\n",
    "print (\"\\033[1;35;46m scatter plot of all the districts(instances) to visulaize the data. \\n \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)\n",
    "plt.show()\n",
    "print (\"\\033[1;35;46m you can see high density of data points by changing the Alpha value.\\n \")\n",
    "print (\"\\033[1;35;46m The bay Area, around los Angeles and San Diego, + a fairly high density in the central valley(Sacramento & Fresno has HIGH DENSITY) \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(housing['population'].head())\n",
    "# Lets look at housing prices.\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n",
    "            s= housing['population'] /100, label='population',\n",
    "            c= housing['median_house_value'], cmap=plt.get_cmap('jet'),colorbar='True',\n",
    "            )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print (\"\\033[1;35;46m clearly tells the house prices are very much related to lacation \\n \")\n",
    "print (\"\\033[1;32;43m QUESTION: How to use clustering Algo to detect \\n 1. the main clusters \\n 2. add new features that measure the proximity to the cluster centers.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### looking at the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computing pearson's r :: standard correlation coeff. between pair of attributes\n",
    "corr_matrix=housing.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_matrix['median_house_value'].sort_values(ascending='False')\n",
    "print (\"\\033[1;35;46m The correlation coeffs. ranges from -1 to 1. \\n  If it is close to 1 : There exist a strong positive correlation. i.e 'median_house_price' tends to go up when the 'median_income goes up, 0.687160 \\n if the coeff. is negative: there exist a strong negative correlation, i.e the 'median_house_price' have the tendency to go down when we move upward in NORTH direction (latitude) \\n The coeff. with near to 0 means that no LINEAR CORRELATION : i.e the x goes UP when the y goes UP/DOWN.\\n It can completely missout the NON-LINEAR RELATIONSHIPS. \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#another way to watch the correlation between attributes.\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "attributes = ['median_house_value','median_income','total_rooms','housing_median_age' ]\n",
    "\n",
    "scatter_matrix(housing[attributes], figsize= (12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='median_income',y='median_house_value',alpha=0.1)\n",
    "plt.show()\n",
    "print (\"\\033[1;35;46m 1. Correlation is very strong.{: can see a upward trend & points are not too dispersed}\\n 2. There is price cap as a horizontal line around $500,000.\\n\\n But the plot reveals other obvious straight lines these are around $350,000,$450,000, $280,000. We may want to remove these DATA QUIRKS to prevent our algorithm from learning it.  \\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Experimenting with various attributes combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(housing.dtypes.index)\n",
    "print (\"\\033[1;35;46m The 'total_rooms' in a district is not useful instead we need # of 'rooms_per_household' \\n\\n  'total_bedrooms' is not useful instead we want to compare it to the # of rooms \\n\\n another intresting attribute is 'population_per_household'  \\n \")\n",
    "#housing['households'].tail()\n",
    "housing['rooms_per_household']     =housing['total_rooms'] / housing['households']\n",
    "housing['bedrooms_per_room']       =housing['total_bedrooms'] / housing['total_rooms']\n",
    "housing['population_per_household']=housing['population'] / housing['households']\n",
    "\n",
    "corr_matrix=housing.corr()\n",
    "print(corr_matrix['median_house_value'].sort_values(ascending=False))\n",
    "\n",
    "print (\"\\033[1;35;46m 'bedrooms_per_room' attribute is more correlated than 'total_bedrooms' & 'total_rooms' :: aparantly, hoses with lower bedroom/room ratio tend to be more expensive.]n Also, it is more informative than 'total_rooms' in a district. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare your data for machine learning algorithms\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the \"PREDICTORs\"\n",
    "\n",
    "housing= strat_train_set.drop('median_house_value',axis=1)\n",
    "\n",
    "# the \"Response\", \"Labels\"\n",
    "housing_labels= strat_train_set['median_house_value'].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most ML algos does not work with \"Missing Values\".\n",
    "# we know that some values we missing\n",
    "\n",
    "housing.info()\n",
    "#Get rid of the corresponding districts.(removing whole row)\n",
    "#housing.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "#Get rid of the whole attribute.\n",
    "#housing.drop('total_bedrooms',axis=1)\n",
    "\n",
    "#Set the values to some value(mean, 0, median etc.)\n",
    "print (\"\\033[1;35;46m  you should compute the median values on training set and and use it to fill the missing values in the training set \\n Also, save the median values you have computed. This will be used to\\n 1.replace the missing values in the testset and \\n 2.to replace missing values in new Data when the system goes live.  \\n\")\n",
    "median =housing['total_bedrooms'].median()\n",
    "nansFilled=housing['total_bedrooms'].fillna(median)\n",
    "nansFilled.size # All 16512 entries retained instead of 16354.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn provides a handy class to handle missing values.\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy= 'median')\n",
    "#Since the median can be computed only on the numerical Attributes, therefore 'ocean_proximity' has to be dropped\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "#now fit the imputer instance to training data: computed the median of each attribute and stored the result in its  'statistics_' instance variable.\n",
    "print (\"\\033[1;35;46m  The reason of computing median on every ATTRIBUTe is that, in future when the system goes live , we might recieve missing values in any atttibute, \\n therefore, we have to be sure \\n\")\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "print(imputer.statistics_)\n",
    "\n",
    "print(housing_num.median().values)\n",
    "#used the trained imputer to transform the training set by replacing the missing values(eg. NANs) by the learned medians.\n",
    "X= imputer.transform(housing_num)\n",
    "print(type(X))\n",
    "# if required tranform 'numpy.ndarray' to pandas's Df.\n",
    "housing_tr=pd.DataFrame(data=X, columns=housing_num.columns)\n",
    "#housing_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NOTE\n",
    "*  **ESTIMATORS: Any object that can estimate some parameters based on datasets _eg. Imputer_. **\n",
    "The esitmation is performed by fit() method and it takes only a **'dataset'** as the parameter or two for supervised Learning.\n",
    "\n",
    " Any other parameter needed to guide the estimation process is **HYPERPARAMETER** _eg. Impute's strategy_ and it **must be set as an instance variable** ,generally, via contructor parameter: always given before training.\n",
    "* **TRANSFORMERS: Some estimators such as _Imputer_ can also transform datasets.**\n",
    " The transformation is performed by transform() method with the **'dataset'** to transform parameter.\n",
    "\n",
    " It returns the transformed dataset. These transformations generally relies on **LEARNED PARAMETERs** _eg. median_,as in case of _Imputer_.\n",
    "* **PREDICTORS: Some estimators are capable of making predictions given a dataset.**\n",
    "a predictor has a predict() method which takes **'dataset'** of a new instance and return a datasets of corresponding predictions.\n",
    "\n",
    " It also has a score() method that measures the **QUALITY of prediction** given a TESTSET(& corresponding labels in terms of supervised learning algorithms.)\n",
    "\n",
    "* All the **estimator's HYPERPARAMETERS** are accessible directly via **public instance variables** _{eg. Imputer.strategy}_ and\n",
    "\n",
    "* All the **estimator's learned parameters** are also accessible via **public instance variables** with an **underscore suffix** _eg. Imputer.statistics_ _  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How to handle text and CATEGORICAL ATTRIBUTES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use a transformer: LabelEncoder.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#create an instance variable\n",
    "encoder = LabelEncoder()\n",
    "housing_cat= housing['ocean_proximity']\n",
    "housing_cat_encoded=encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded.shape\n",
    "print (\"\\033[1;35;46m  Now we can use this numerical data in any ML algorthims.\")\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"\\033[1;35;46m Issue with above encoding scheme: The ML algorithms will assume that the two nearby values are more similar than two distant values.\\n{eg. for 'ocean_proximity' 0 and 4 looks more similar than 0 and 1 }\\n\\n SOLUTION: create one BINARY attribute per category:ONE HOT ENCODING.\\n\\n 1:HOT, 0:COLD \")\n",
    "from sklearn.preprocessing import OneHotEncoder # converts INTEGER categorical values to oneHot vectors.\n",
    "hot_encoder= OneHotEncoder()\n",
    "housing_cat_1hot=hot_encoder.fit_transform(housing_cat_encoded.reshape(-1,1))# fit_transform() expect a 2D array so we need to reshape our 1D array.\n",
    "housing_cat_1hot\n",
    "type(housing_cat_1hot)# scipy sparse matrix : \n",
    "print (\"\\033[1;35;46m The benefit of using scipy sparce matrix intead of NumPy array(dense), because sparse representation stores the location of nonzero elements(in our case 1). \")\n",
    "print (\"\\033[1;32;43m QUESTION:What is the reason behind that the ML algorithms assume that two nearby values are more similar than two distant values. \")\n",
    "\n",
    "# to convert it to NumPy array (dense representation.)\n",
    "housing_cat_1hot.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRANSFORMATION: text cat.--> Integer cat.--> One Hot vectors.\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder_OneHot=LabelBinarizer()# pass ~~ sparse_output='True' ~~ to get sciPy sparse matrix\n",
    "housing_cat_oneHot= encoder_OneHot.fit_transform(housing_cat) #return a dense NumPy array\n",
    "housing_cat_oneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You need to write your own TRANSFORMERS for task such as CUSTOM CLEANUP OPERATIONS or  COMBINING SPECIFIC ATTRIBUTES.\n",
    "# Need to create a class and implement 3 methods.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#assigning column values : 'total_rooms'=3, 'total_bedrooms'=4, 'population'=5, 'households'=6\n",
    "room_ix, bedrooms_ix, population_ix, household_ix = 3,4,5,6\n",
    "class combinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedroom_per_room =True):#\n",
    "        self.add_bedroom_per_room = add_bedroom_per_room\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def tranform(self, X, y=None):\n",
    "        room_per_household = X[:, room_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedroom_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, room_ix]\n",
    "            return np.c_[X, room_per_household, population_per_household, bedrooms_per_room ]\n",
    "        else:\n",
    "            return np.c_[X, room_per_household, population_per_household]\n",
    "        \n",
    "\n",
    "attr_adder = combinedAttributesAdder(add_bedroom_per_room = False)\n",
    "housing_extra_attri=attr_adder.tranform(housing.values)\n",
    "housing_extra_attri.shape # 9 + two more columns have been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Feature Scaling\n",
    "ML algos **does not perfrom** well when the input numerical attributes have very different **scales**.\n",
    "Note : Scaling the target values, Generally, not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumy1, dumy2 = housing.total_rooms.unique(),housing.median_income.unique()\n",
    "print(housing.total_rooms.min(),\"-->\", housing.total_rooms.max(),\"\\n\")\n",
    "print(np.floor(housing.median_income.min()),\"-->\", np.floor(housing.median_income.max()),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are two methods for scaling:\n",
    "* **Min-Max scaling / NORMALIZATION**: **every value** if **substracted** by **Min. value** and **divided** by **Min. - Max**. The values are shifted and rescaled so that they end up ranging from 0 to 1.\n",
    "* **STANDARDIZATION** : **every value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
